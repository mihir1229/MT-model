{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f624609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b8fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e750d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (0.1.97)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: portalocker in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from sacrebleu) (2.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from sacrebleu) (0.4.5)\n",
      "Requirement already satisfied: lxml in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from sacrebleu) (4.9.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from sacrebleu) (0.8.10)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from portalocker->sacrebleu) (302)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers sentencepiece sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1d7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint=\"facebook/nllb-200-distilled-600M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7744aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.src_lang = \"ory_Orya\"  \n",
    "tokenizer.tgt_lang = \"hin_Deva\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44dbeb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_train=[]\n",
    "hindi_test=[]\n",
    "odia_train=[]\n",
    "odia_test=[]\n",
    "with open(\"odia_train3.0.txt\",'r',encoding=\"utf8\") as f1, open(\"odia_test3.0.txt\",'r',encoding=\"utf8\") as f2, open(\"hindi_train3.0.txt\",'r',encoding=\"utf8\") as f3, open(\"hindi_test3.0.txt\",'r',encoding=\"utf8\") as f4:\n",
    "    for l in f1:\n",
    "        line=l[:-1]\n",
    "        odia_train.append(line)\n",
    "    for l in f2:\n",
    "        line=l[:-1]\n",
    "        odia_test.append(line)\n",
    "    for l in f3:\n",
    "        line=l[:-1]\n",
    "        hindi_train.append(line)\n",
    "    for l in f4:\n",
    "        line=l[:-1]\n",
    "        hindi_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8844fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8474bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in range(len(hindi_train)):\n",
    "    c={}\n",
    "    c['translation']={}\n",
    "    c['translation']['ory_Orya']=odia_train[i]\n",
    "    c['translation']['hin_Deva']=hindi_train[i]\n",
    "    data.append(dict(c))\n",
    "\n",
    "\n",
    "f1=open(\"train.json\",\"a\")\n",
    "for i in range(len(data)):\n",
    "    obj=json.dumps(data[i])\n",
    "    f1.write(obj+'\\n')\n",
    "\n",
    "    \n",
    "data=[]\n",
    "for i in range(len(hindi_test)):\n",
    "    c={}\n",
    "    c['translation']={}\n",
    "    c['translation']['ory_Orya']=odia_train[i]\n",
    "    c['translation']['hin_Deva']=hindi_train[i]\n",
    "    data.append(dict(c))\n",
    "\n",
    "f1=open(\"test.json\",\"a\")\n",
    "for i in range(len(data)):\n",
    "    obj=json.dumps(data[i])\n",
    "    f1.write(obj+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e1848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c5ca58a6a802bfcf\n",
      "C:\\Users\\Muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:\\Users\\Muktikanta\\.cache\\huggingface\\datasets\\json\\default-c5ca58a6a802bfcf\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2008.29it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 669.70it/s]\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:\\Users\\Muktikanta\\.cache\\huggingface\\datasets\\json\\default-c5ca58a6a802bfcf\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 32.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "raw_datasets = load_dataset(\"json\", data_files={\"train\":\"train.json\",\"validation\":\"test.json\"})\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae6c996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 25770\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 11032\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dbff342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'ory_Orya': 'ପଞ୍ଜାବ-ହରିୟାଣା ସମେତ ଅନେକ ରାଜ୍ୟର କୃଷକମାନେ ଜାତୀୟ ରାଜଧାନୀ ଦିଲ୍ଲୀର ସୀମାରେ କେନ୍ଦ୍ର ସରକାରଙ୍କ ନୂତନ କୃଷି ନିୟମକୁ ବିରୋଧ କରୁଛନ୍ତି।',\n",
       "  'hin_Deva': 'केंद्र सरकार के नए कृषि कानूनों को लेकर पंजाब-हरियाणा समेत कई राज्यों के किसान राष्ट्रीय राजधानी दिल्ली की सीमाओं पर धरना प्रदर्शन कर रहे हैं ।'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6b278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint == \"od-hi-nllb-fine-tune-checkpoint\":\n",
    "    prefix = \"translate Odia to Hindi: \"\n",
    "else:\n",
    "    prefix = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d1e1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"ory_Orya\"\n",
    "target_lang = \"hin_Deva\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82edf967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x0000014C315FDD30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|                                                                                           | 0/26 [00:00<?, ?ba/s]C:\\Users\\Muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:03<00:00,  8.32ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00,  9.35ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56b59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cceabced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 12 16:21:48 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.96                 Driver Version: 536.96       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000              WDDM  | 00000000:17:00.0 Off |                  Off |\n",
      "| 33%   32C    P8              14W / 260W |   1607MiB / 49152MiB |     11%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     11932    C+G   ...n\\120.0.2210.144\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     20332    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     20636    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     20880    C+G   ....Cortana_cw5n1h2txyewy\\SearchUI.exe    N/A      |\n",
      "|    0   N/A  N/A     31992    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     32792    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4ecbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 38\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=60,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb8350b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3195834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfc279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d055586",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    "    \n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bda36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12761' max='40740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12761/40740 24:23:00 < 53:28:12, 0.15 it/s, Epoch 18.79/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>0.988998</td>\n",
       "      <td>38.487700</td>\n",
       "      <td>31.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.252500</td>\n",
       "      <td>0.910395</td>\n",
       "      <td>40.911500</td>\n",
       "      <td>31.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.128800</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>42.437800</td>\n",
       "      <td>31.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.069500</td>\n",
       "      <td>0.797443</td>\n",
       "      <td>44.198800</td>\n",
       "      <td>31.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.035700</td>\n",
       "      <td>0.750310</td>\n",
       "      <td>45.796100</td>\n",
       "      <td>31.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.977300</td>\n",
       "      <td>0.708208</td>\n",
       "      <td>46.968600</td>\n",
       "      <td>31.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.932700</td>\n",
       "      <td>0.671238</td>\n",
       "      <td>48.272700</td>\n",
       "      <td>31.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.909700</td>\n",
       "      <td>0.634377</td>\n",
       "      <td>49.799800</td>\n",
       "      <td>31.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.857700</td>\n",
       "      <td>0.602178</td>\n",
       "      <td>51.306100</td>\n",
       "      <td>31.867400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.572397</td>\n",
       "      <td>52.643000</td>\n",
       "      <td>31.838400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.806600</td>\n",
       "      <td>0.543595</td>\n",
       "      <td>54.137400</td>\n",
       "      <td>31.819400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>0.515605</td>\n",
       "      <td>55.562900</td>\n",
       "      <td>31.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.490445</td>\n",
       "      <td>56.809500</td>\n",
       "      <td>31.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.466799</td>\n",
       "      <td>58.065200</td>\n",
       "      <td>31.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.443728</td>\n",
       "      <td>59.222500</td>\n",
       "      <td>31.910200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>60.570700</td>\n",
       "      <td>31.989700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.402315</td>\n",
       "      <td>62.001200</td>\n",
       "      <td>31.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.383082</td>\n",
       "      <td>63.247100</td>\n",
       "      <td>31.970500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37f446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to translate: ପ୍ରାରମ୍ଭିକ ଦିନରେ ଅଛି।\n",
      "Translated sentence: शुरुआती दिनों में है।\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# # Define the checkpoint path\n",
    "# checkpoint_path = \"C:\\\\Users\\\\Muktikanta\\\\nllb_odi_hindi\\\\nllb-200-distilled-600M-finetuned-ory_Orya-to-hin_Deva\\\\checkpoint-12500\"\n",
    "\n",
    "# # Load the trained model checkpoint\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(model, open('model.pkl','wb'))\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl','wb'))\n",
    "# import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Prompt the user to type a sentence\n",
    "input_sentence = input(\"Enter a sentence to translate: \")\n",
    "\n",
    "# Translate the input sentence\n",
    "input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
    "translated_ids = model.generate(input_ids)\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the translated text\n",
    "print(\"Translated sentence:\", translated_text)\n",
    "\n",
    "\n",
    "# with open('model.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "# with open('tokenizer.pkl', 'rb') as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "# # Input file path containing sentences to be translated\n",
    "# input_file_path = \"C:\\\\Users\\\\Muktikanta\\\\nllb_distilled-600M-odia-hindi\\\\1-10_sentence.txt\"\n",
    "\n",
    "# # Output file path to save the translated sentences\n",
    "# output_file_path = \"C:\\\\Users\\\\Muktikanta\\\\nllb_distilled-600M-odia-hindi\\\\model_translated_text.txt\"\n",
    "\n",
    "# # Read sentences from the input file\n",
    "# with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "#     input_sentences = input_file.readlines()\n",
    "\n",
    "# # Translate each input sentence and save the translations to the output file\n",
    "# with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "#     for input_sentence in input_sentences:\n",
    "#         input_sentence = input_sentence.strip()\n",
    "#         input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
    "#         translated_ids = model.generate(input_ids)\n",
    "#         translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "#         output_file.write(translated_text + \"\\n\")\n",
    "\n",
    "# print(\"Translation complete. Translated sentences saved to:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(source_text):\n",
    "    # Replace this with your translation logic using the loaded model\n",
    "    input_ids = tokenizer.encode(source_text, return_tensors=\"pt\")\n",
    "    translated_ids = translation_model.generate(input_ids)\n",
    "    translated_token = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    translated_text = tokenizer.translate(source_text)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84aee675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (4.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18da5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.21.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55292e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (4.21.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp39-none-win_amd64.whl (269 kB)\n",
      "     ---------------------------------------- 269.7/269.7 kB ? eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.9.1\n",
      "    Uninstalling huggingface-hub-0.9.1:\n",
      "      Successfully uninstalled huggingface-hub-0.9.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.2\n",
      "    Uninstalling transformers-4.21.2:\n",
      "      Successfully uninstalled transformers-4.21.2\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.20.3 safetensors-0.4.2 tokenizers-0.15.1 transformers-4.37.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e9bc8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.21.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3dafce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.15.0\n",
      "  Downloading tensorflow_intel-2.15.0-cp39-cp39-win_amd64.whl (300.8 MB)\n",
      "     -------------------------------------- 300.8/300.8 MB 4.5 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.1-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 47.6 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 92.3 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 34.6 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 39.2 MB/s eta 0:00:00\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.3.0)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "     ---------------------------------------- 442.0/442.0 kB ? eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Collecting numpy<2.0.0,>=1.23.5\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "     --------------------------------------- 15.8/15.8 MB 40.9 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     --------------------------------------- 24.4/24.4 MB 26.2 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl (938 kB)\n",
      "     -------------------------------------- 938.4/938.4 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 18.1 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 133.7/133.7 kB ? eta 0:00:00\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.2-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     ------------------------------------- 413.4/413.4 kB 25.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "     ------------------------------------- 226.7/226.7 kB 14.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.4-cp39-cp39-win_amd64.whl (422 kB)\n",
      "     ------------------------------------- 422.5/422.5 kB 25.8 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "     ------------------------------------- 186.8/186.8 kB 11.0 MB/s eta 0:00:00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Muktikanta\\\\.conda\\\\envs\\\\pytorch-gpu\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "     ---------------------------------------- 103.9/103.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from packaging->tensorflow-intel==2.15.0->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.3/181.3 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\muktikanta\\.conda\\envs\\pytorch-gpu\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.8.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.9/84.9 kB ? eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, opt-einsum, ml-dtypes, markdown, h5py, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecc0357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spyder 5.2.2 requires pyqtwebengine, which is not installed.Note: you may need to restart the kernel to use updated packages.\n",
      "spyder 5.2.2 has requirement pyqt5<5.13, but you have pyqt5 5.15.7.\n",
      "scipy 1.9.3 has requirement numpy<1.26.0,>=1.18.5, but you have numpy 1.26.4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b897b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"deptree\"\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c5683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
